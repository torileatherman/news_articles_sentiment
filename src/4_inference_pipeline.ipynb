{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Pipeline\n",
    "\n",
    "The inference pipeline is used to predict the sentiment classification for batch data. The pretrained model is downloaded from Hopsworks, and the most recent batch data is downloaded from HuggingFace. After prediction, the data is sorted based on the confidence of the prediction and re-saved on HuggingFace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hopsworks\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import numpy as np\n",
    "from huggingface_hub import notebook_login\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn import preprocessing as p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Hopsworks and download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy your Api Key (first register/login): https://c.app.hopsworks.ai/account/api/generated\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/5321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "project = hopsworks.login()\n",
    "fs = project.get_feature_store()\n",
    "mr = project.get_model_registry()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LSTM Model from Hopsworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file ... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-15 13:03:26.317233: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-01-14 11:15:52         2496\n",
      "metadata.json                                  2023-01-14 11:15:52           64\n",
      "variables.h5                                   2023-01-14 11:15:52      4751728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-15 13:03:46.649713: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dropout\n",
      ".........vars\n",
      "......dropout_1\n",
      ".........vars\n",
      "......embedding\n",
      ".........vars\n",
      "............0\n",
      "......lstm\n",
      ".........cell\n",
      "............vars\n",
      "...............0\n",
      "...............1\n",
      "...............2\n",
      ".........vars\n",
      "...metrics\n",
      "......mean\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......mean_metric_wrapper\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........10\n",
      ".........11\n",
      ".........12\n",
      ".........2\n",
      ".........3\n",
      ".........4\n",
      ".........5\n",
      ".........6\n",
      ".........7\n",
      ".........8\n",
      ".........9\n",
      "...vars\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 60, 40)            200000    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 60, 40)            0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 200)               192800    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 603       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 393,403\n",
      "Trainable params: 393,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = mr.get_model(\"headlines_sentiment_model\", version=2)\n",
    "model_dir = model.download()\n",
    "model = joblib.load(model_dir + \"/headlines_sentiment_model.pkl\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect and load batch data from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fbedde6c65b4b869da459c6098c7761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-15 13:04:36,564 WARNING: Using custom data configuration eengel7--sentiment_analysis_batch-524b74c4b113f844\n",
      "Downloading and preparing dataset parquet/eengel7--sentiment_analysis_batch to /Users/torileatherman/.cache/huggingface/datasets/eengel7___parquet/eengel7--sentiment_analysis_batch-524b74c4b113f844/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e7594bb7897430b8f0903a44d12318f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7ef7560fe2a4043b670f794661eec07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/14.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d748c167298f4c799748804fcd04dc25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6feb8d6c2e5c4903a8e055a851851c76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /Users/torileatherman/.cache/huggingface/datasets/eengel7___parquet/eengel7--sentiment_analysis_batch-524b74c4b113f844/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "# Conneect to HuggingFace\n",
    "notebook_login()\n",
    "\n",
    "# Load scraped batch data from HuggingFace\n",
    "batch_data = load_dataset(\"eengel7/sentiment_analysis_batch\",split='train')\n",
    "batch_data = pd.DataFrame(data=batch_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict sentiment of batch data and sort based on confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 107ms/step\n"
     ]
    }
   ],
   "source": [
    "# Get classification probability for every label\n",
    "y_predictions = model.predict(batch_data['Headline'].to_list())\n",
    "\n",
    "# Normalize classification probabilities for each headline\n",
    "y_predictions = p.normalize(y_predictions, norm ='l1')\n",
    "\n",
    "# Set highest classification probability as label\n",
    "batch_data['Prediction'] = y_predictions.argmax(axis=1)\n",
    "\n",
    "# Sort batch data depending on classification confidence\n",
    "batch_data['Confidence'] = np.amax(y_predictions, axis=1)\n",
    "batch_data = batch_data.sort_values('Confidence', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct dataframe and upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc224f71237f4abcb685e64a282d0bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42096776fb8545738571ee3e9b97800a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Upload to HuggingFace\u001b[39;00m\n\u001b[1;32m      2\u001b[0m batch_predictions_dataset \u001b[39m=\u001b[39m Dataset\u001b[39m.\u001b[39mfrom_pandas(batch_data)\n\u001b[0;32m----> 3\u001b[0m batch_predictions_dataset\u001b[39m.\u001b[39;49mpush_to_hub(\u001b[39m\"\u001b[39;49m\u001b[39mtorileatherman/sentiment_analysis_batch_predictions\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/Github/ID2223_scalable_machine_learning/news_articles_sentiment/.venv/lib/python3.8/site-packages/datasets/arrow_dataset.py:4966\u001b[0m, in \u001b[0;36mDataset.push_to_hub\u001b[0;34m(self, repo_id, split, private, token, branch, max_shard_size, num_shards, shard_size, embed_external_files)\u001b[0m\n\u001b[1;32m   4964\u001b[0m     dataset_metadata \u001b[39m=\u001b[39m DatasetMetadata\u001b[39m.\u001b[39mfrom_readme(Path(dataset_readme_path))\n\u001b[1;32m   4965\u001b[0m     dataset_infos: DatasetInfosDict \u001b[39m=\u001b[39m DatasetInfosDict\u001b[39m.\u001b[39mfrom_metadata(dataset_metadata)\n\u001b[0;32m-> 4966\u001b[0m     repo_info \u001b[39m=\u001b[39m dataset_infos[\u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(dataset_infos))]\n\u001b[1;32m   4967\u001b[0m \u001b[39m# get the deprecated dataset_infos.json to uodate them\u001b[39;00m\n\u001b[1;32m   4968\u001b[0m \u001b[39melif\u001b[39;00m config\u001b[39m.\u001b[39mDATASETDICT_INFOS_FILENAME \u001b[39min\u001b[39;00m repo_files:\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Upload to HuggingFace\n",
    "batch_predictions_dataset = Dataset.from_pandas(batch_data)\n",
    "batch_predictions_dataset.push_to_hub(\"torileatherman/sentiment_analysis_batch_predictions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88cf0b9956f47715b2e9c5665ed59c21bcaaf43e7c7b35220cd69262c4bb76dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
